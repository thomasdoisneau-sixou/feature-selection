{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection on DOROTHEA\n",
    "\n",
    "### Steps in the code\n",
    "1) Importing the dataset and loading all relevant packages, grouped by their function in our code.\n",
    "2) Exploratory data analysis, producing Figures 1 and 2 for our report.\n",
    "3) Embedded methods and filters for Section 2 of our report.\n",
    "4) Line chart to illustrate the results from the previous step.\n",
    "5) Forward Stepwise Selection and Backward Stepwise Selection.\n",
    "6) FSS / BSS plot (this plot is omitted from the report).\n",
    "7) Mutual Information + Lasso + SVM\n",
    "8) Optimized Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Preliminaries\n",
    "# Import necessary libraries and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import time\n",
    "import ast\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pympler.asizeof import asizeof\n",
    "from brokenaxes import brokenaxes\n",
    "\n",
    "# Data handling and splitting\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import (\n",
    "    RFE, SelectKBest, SelectFromModel, SequentialFeatureSelector,\n",
    "    mutual_info_classif, VarianceThreshold, f_classif, chi2\n",
    ")\n",
    "from skrebate import ReliefF\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "#Loading data from repository\n",
    "data_raw = pd.read_csv(\"data2.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Exploratory data analysis\n",
    "#We plot two figures to represent the data. Additionally, we calculate a few metric that describe the data, its size and its properties.\n",
    "\n",
    "# Figure 1: Class imbalance with percentages inside bars\n",
    "label_counts = data_raw['label'].value_counts()\n",
    "total_count = label_counts.sum()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bar_positions = [0.25, 0.75]\n",
    "plt.bar(bar_positions, label_counts, color=['lightgrey', 'darkgrey'], edgecolor=\"black\", width=0.4)\n",
    "for pos, count in zip(bar_positions, label_counts):\n",
    "    plt.text(pos, count - total_count * 0.05, f\"{(count / total_count) * 100:.1f}%\", \n",
    "             ha='center', va='center', fontsize=10, color='black')\n",
    "plt.xlabel(\"Compound Activity\")\n",
    "plt.ylabel(\"Number of Compounds\")\n",
    "plt.xticks(bar_positions, [\"Inactive (-1)\", \"Active (1)\"])\n",
    "plt.xlim(0, 1)\n",
    "plt.savefig(\"Plots/figure_1.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: Distribution of means of all features\n",
    "plt.figure(figsize=(6, 4))\n",
    "column_means = data_raw.mean()\n",
    "proportion_ones = (data_raw['label'] == 1).mean()\n",
    "\n",
    "plt.hist(column_means, bins=500, color='lightgrey', edgecolor='darkgrey')\n",
    "plt.axvline(proportion_ones, color='black', linestyle='--', linewidth=1)\n",
    "plt.text(proportion_ones - 0.01, plt.gca().get_ylim()[1] * 0.8, \n",
    "         f\"Proportion of positive class\\nin target: {proportion_ones:.3f}\", \n",
    "         color='black', fontsize=10, ha='right', va='center')\n",
    "plt.xlabel(\"Mean Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(0, 0.15)\n",
    "plt.savefig(\"Plots/figure_2.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "#Sparsity score: number of non-zero elements / total number of elements in matrix\n",
    "print(f\"Data has a sparsity score of: {round(1 - csr_matrix(data_raw).nnz/(csr_matrix(data_raw).shape[0] * csr_matrix(data_raw).shape[1]), 3)*100}%.\")\n",
    "\n",
    "#Size of dataset: comparison between sparse and dense representation\n",
    "print(f\"Dense representation is {round(asizeof(data_raw)/asizeof(csr_matrix(data_raw)))} times larger in memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Testing filters and embedded methods with 10-fold cross-validation and separate test set\n",
    "\n",
    "#Defining values for number of features to select\n",
    "values = np.concatenate([\n",
    "    np.arange(1, 21, 1),        # Steps of 1 from 1 to 20\n",
    "    np.arange(20, 101, 20),     # Steps of 20 from 20 to 100\n",
    "    np.arange(100, 501, 50),    # Steps of 50 from 100 to 500\n",
    "    np.arange(500, 1001, 100)   # Steps of 100 from 500 to 1000\n",
    "])\n",
    "values_list = list(values)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Define hyperparameter ranges for each method\n",
    "hyperparameters = {\n",
    "    \"VarianceThreshold\": {\"threshold\": np.linspace(0.05, 0.2, 50)},\n",
    "    \"Correlation (f_classif)\": {\"k\": values_list},\n",
    "    #\"Mutual Information\": {\"k\": values_list},\n",
    "    \"Chi-Square\": {\"k\": values_list},\n",
    "    \"RandomForest\": {\"threshold\": np.union1d(np.linspace(1e-7, 1e-3, 50), np.logspace(-3, -2, 10, base=10))},\n",
    "    \"Lasso\": {\"C\": sorted(set(np.arange(0.01, 0.3, 0.01)).union({0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 3, 5, 10, 20, 50, 100, 1000}))},\n",
    "    #\"ReliefF\": {\"n_features_to_select\": values_list},\n",
    "    #\"ElasticNet\": {\"C\": sorted(set(np.arange(0.01, 0.3, 0.01)).union({0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 3, 5, 10, 20, 50, 100, 1000}))},\n",
    "    #\"mRMR\": {\"k\": values_list}\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=25)\n",
    "\n",
    "# Convert X and y to arrays\n",
    "X = data_raw.iloc[:, 1:].values\n",
    "y = data_raw['label']\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = csr_matrix(X_train)\n",
    "X_test = csr_matrix(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Start hyperparameter search for each filter method\n",
    "for method_name, params in hyperparameters.items():\n",
    "    for param, values in params.items():\n",
    "        for value in tqdm(values, desc=f\"{method_name} Progress ({param})\"):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                balanced_accuracies = []  # Store CV accuracies\n",
    "\n",
    "                # Perform cross-validation\n",
    "                for train_idx, test_idx in cv.split(X_train, y_train):\n",
    "                    # Split the data\n",
    "                    X_train_fold, X_test_fold = X_train[train_idx], X_train[test_idx]\n",
    "                    y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "                    # Initialize filter method with current hyperparameter\n",
    "                    if method_name == \"VarianceThreshold\":\n",
    "                        method = VarianceThreshold(threshold=value)\n",
    "                        method.fit(X_train_fold.toarray())\n",
    "                        selected_indices = np.where(method.get_support())[0]\n",
    "                    elif method_name == \"Correlation (f_classif)\":\n",
    "                        method = SelectKBest(score_func=f_classif, k=value)\n",
    "                        method.fit(X_train_fold.toarray(), y_train_fold)\n",
    "                        selected_indices = method.get_support(indices=True)\n",
    "                    elif method_name == \"Mutual Information\":\n",
    "                        method = SelectKBest(score_func=mutual_info_classif, k=value)\n",
    "                        method.fit(X_train_fold.toarray(), y_train_fold)\n",
    "                        selected_indices = method.get_support(indices=True)\n",
    "                    elif method_name == \"Chi-Square\":\n",
    "                        method = SelectKBest(score_func=chi2, k=value)\n",
    "                        method.fit(X_train_fold.toarray(), y_train_fold)\n",
    "                        selected_indices = method.get_support(indices=True)\n",
    "                    elif method_name == \"ReliefF\":\n",
    "                        chi2_filter = SelectKBest(score_func=chi2, k=1000)\n",
    "                        X_chi2_train = chi2_filter.fit_transform(X_train_fold.toarray(), y_train_fold)\n",
    "                        method = ReliefF(n_features_to_select=value)\n",
    "                        method.fit(X_chi2_train, y_train_fold)\n",
    "                        selected_indices = method.top_features_[:value]\n",
    "                    elif method_name == \"RandomForest\":\n",
    "                        method = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=value)\n",
    "                        method.fit(X_train_fold.toarray(), y_train_fold)\n",
    "                        selected_indices = method.get_support(indices=True)\n",
    "                    elif method_name == \"Lasso\":\n",
    "                        lasso = LogisticRegression(penalty='l1', solver='saga', C=value, max_iter=10000, random_state=42)\n",
    "                        lasso.fit(X_train_fold, y_train_fold)\n",
    "                        y_pred = lasso.predict(X_test_fold)\n",
    "                        balanced_accuracies.append(balanced_accuracy_score(y_test_fold, y_pred))\n",
    "                        selected_indices = np.where(lasso.coef_[0] != 0)[0]\n",
    "                        continue\n",
    "                    elif method_name == \"ElasticNet\":\n",
    "                        elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', C=value, max_iter=10000, random_state=42, l1_ratio=0.8)\n",
    "                        elasticnet.fit(X_train_fold, y_train_fold)\n",
    "                        y_pred = elasticnet.predict(X_test_fold)\n",
    "                        balanced_accuracies.append(balanced_accuracy_score(y_test_fold, y_pred))\n",
    "                        selected_indices = np.where(elasticnet.coef_[0] != 0)[0]\n",
    "                        continue\n",
    "                    elif method_name == \"mRMR\":\n",
    "                        X_train_dense = X_train_fold.toarray()\n",
    "                        X_train_df = pd.DataFrame(X_train_dense)\n",
    "                        selected_indices = mrmr_classif(X_train_df, pd.Series(y_train_fold), K=value)\n",
    "\n",
    "                    # Filter features using the selected indices\n",
    "                    X_train_filtered = X_train_fold[:, selected_indices]\n",
    "                    X_test_filtered = X_test_fold[:, selected_indices]\n",
    "\n",
    "                    # Train logistic regression on filtered features (except for Lasso and ElasticNet)\n",
    "                    model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "                    model.fit(X_train_filtered.toarray(), y_train_fold)\n",
    "                    y_pred = model.predict(X_test_filtered.toarray())\n",
    "                    balanced_accuracies.append(balanced_accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "                # Log results\n",
    "                mean_balanced_acc = np.mean(balanced_accuracies)\n",
    "                num_features = len(selected_indices)\n",
    "                runtime = time.time() - start_time\n",
    "\n",
    "                results.append({\n",
    "                    \"Method\": method_name,\n",
    "                    \"Parameter\": param,\n",
    "                    \"Value\": value,\n",
    "                    \"Balanced Accuracies (CV)\": balanced_accuracies,\n",
    "                    \"Mean Balanced Accuracy\": mean_balanced_acc,\n",
    "                    \"Number of Features Selected\": num_features,\n",
    "                    \"Selected Features\": selected_indices,\n",
    "                    \"Runtime (seconds)\": runtime,\n",
    "                })\n",
    "\n",
    "                print(f\"Mean Balanced Accuracy: {mean_balanced_acc:.4f}, Features: {num_features}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name}, {param}: {value} - {e}\")\n",
    "\n",
    "# Save results to a DataFrame and CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"combined_filter_methods_results.csv\", index=False)\n",
    "\n",
    "# Evaluate the best configuration of each method on the test set\n",
    "final_results = []\n",
    "\n",
    "# Iterate through methods and their best configurations\n",
    "for method_name in hyperparameters.keys():\n",
    "    # Find the best configuration for this method\n",
    "    method_results = [res for res in results if res[\"Method\"] == method_name]\n",
    "    if not method_results:\n",
    "        continue  # Skip if no results for this method\n",
    "    \n",
    "    best_result = max(method_results, key=lambda x: x[\"Mean Balanced Accuracy\"])\n",
    "    best_param = best_result[\"Parameter\"]\n",
    "    best_value = best_result[\"Value\"]\n",
    "    selected_indices = best_result[\"Selected Features\"]\n",
    "\n",
    "    # Filter features for the entire train and test set\n",
    "    X_train_best = X_train[:, selected_indices]\n",
    "    X_test_best = X_test[:, selected_indices]\n",
    "\n",
    "    # Train the final model on the entire training set\n",
    "    final_model = LogisticRegression(random_state=25, max_iter=10000)\n",
    "    final_model.fit(X_train_best.toarray(), y_train)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_test_pred = final_model.predict(X_test_best.toarray())\n",
    "    test_balanced_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Log final test results for this method\n",
    "    final_results.append({\n",
    "        \"Method\": method_name,\n",
    "        \"Best Parameter\": best_param,\n",
    "        \"Best Value\": best_value,\n",
    "        \"Balanced Accuracy on Test Set\": test_balanced_acc,\n",
    "        \"Number of Features Selected\": len(selected_indices)\n",
    "    })\n",
    "\n",
    "    print(f\"\\n{method_name} - Test Set Results:\")\n",
    "    print(f\"Best Parameter: {best_param}\")\n",
    "    print(f\"Best Value: {best_value}\")\n",
    "    print(f\"Balanced Accuracy on Test Set: {test_balanced_acc:.4f}\")\n",
    "    print(f\"Number of Features Selected: {len(selected_indices)}\")\n",
    "\n",
    "# Save final results to a CSV for test evaluation\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "final_results_df.to_csv(\"test_set_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nFinal Test Set Evaluation Results Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Linechart for filter and embedded methods\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the results\n",
    "results_df = pd.read_csv(\"combined_filter_methods_results.csv\")\n",
    "\n",
    "# Extract and compute standard deviation from the \"Balanced Accuracies (CV)\" column\n",
    "results_df[\"Balanced Accuracies (CV)\"] = results_df[\"Balanced Accuracies (CV)\"].apply(ast.literal_eval)  # Convert string to list\n",
    "results_df[\"Balanced Accuracy Std\"] = results_df[\"Balanced Accuracies (CV)\"].apply(lambda x: pd.Series(x).std())\n",
    "results_df[\"Mean Balanced Accuracy\"] = results_df[\"Balanced Accuracies (CV)\"].apply(lambda x: pd.Series(x).mean())\n",
    "\n",
    "# Define the data\n",
    "methods = results_df[\"Method\"]\n",
    "\n",
    "# Create subplots with defined width ratios\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, facecolor='w',\n",
    "                              gridspec_kw={'width_ratios': (1, 2), 'wspace': 0.02},\n",
    "                              figsize=(16, 8))\n",
    "\n",
    "# Define feature ranges for each subplot\n",
    "ranges = [(0, 50), (51, 1000)]\n",
    "\n",
    "# Plot data with mean lines and error bars at highest mean accuracy points\n",
    "for ax, (xmin, xmax) in zip([ax1, ax2], ranges):\n",
    "    for method in methods.unique():\n",
    "        method_data = results_df[results_df[\"Method\"] == method]\n",
    "        x = method_data[\"Number of Features Selected\"]\n",
    "        y = method_data[\"Mean Balanced Accuracy\"]\n",
    "        yerr = method_data[\"Balanced Accuracy Std\"]\n",
    "\n",
    "        # Plot the mean line\n",
    "        line, = ax.plot(x, y, label=method, linewidth=2)\n",
    "\n",
    "        # Find the point with the highest mean accuracy\n",
    "        max_idx = y.idxmax()\n",
    "        max_x = x[max_idx]\n",
    "        max_y = y[max_idx]\n",
    "        max_yerr = yerr[max_idx]\n",
    "\n",
    "        # Mark the point with an error bar\n",
    "        ax.errorbar(\n",
    "            max_x, max_y, yerr=max_yerr,\n",
    "            fmt='o', color=line.get_color(), label=None, capsize=5\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "\n",
    "# Turn off the axes in the combined plot\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.tick_params(axis='y', length=0)\n",
    "\n",
    "# Add labels and title\n",
    "f.text(0.5, 0.04, \"Number of Features Selected\", ha='center', fontsize=16)\n",
    "f.text(0.07, 0.5, \"Mean Balanced Accuracy\", va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "# Move the legend to the bottom left corner inside the plot area\n",
    "ax2.legend(\n",
    "    title=\"Method\",\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(0.999, 0.65),\n",
    "    fontsize=14,\n",
    "    title_fontsize=16,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/figure_3_2.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Forward and backward selection\n",
    "#We use forward and backward sequential selection on a pre-filtered subset of the data as the algorithm is otherwise not computationally feasible.\n",
    "\n",
    "#Prepare training and test set, prepare table to store results\n",
    "X = data_raw.iloc[:, 1:]\n",
    "y = data_raw['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train_sparse = csr_matrix(X_train)\n",
    "X_test_sparse = csr_matrix(X_test)\n",
    "\n",
    "results_table = pd.DataFrame(columns=['Method', 'Balanced Accuracy', 'Number of Features Selected', 'Runtime (seconds)', 'Selected Features'])\n",
    "\n",
    "#Just Chi2 filter (for comparison)\n",
    "for i in tqdm(range(10, 100, 10)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Apply Chi2 filter\n",
    "    chi_selector = SelectKBest(score_func=chi2, k=i)\n",
    "    X_train_chi_selected = chi_selector.fit_transform(X_train_sparse, y_train)\n",
    "    X_test_chi_selected = chi_selector.transform(X_test_sparse)\n",
    "    selected_features = chi_selector.get_support(indices=True)\n",
    "\n",
    "    # Train and evaluate model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_chi_selected, y_train)\n",
    "    y_pred = model.predict(X_test_chi_selected)\n",
    "    runtime = time.time() - start_time\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_table = pd.concat([results_table, pd.DataFrame({\n",
    "        'Method': ['Chi2'],\n",
    "        'Balanced Accuracy': [balanced_acc],\n",
    "        'Number of Features Selected': [len(selected_features)],\n",
    "        'Runtime (seconds)': [runtime],\n",
    "        'Selected Features': [selected_features.tolist()]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Variance Threshold, then Forward Stepwise Selection (FSS)\n",
    "variance_selector = VarianceThreshold(threshold=0.09)\n",
    "X_train_var_selected = variance_selector.fit_transform(X_train_sparse.toarray())\n",
    "X_test_var_selected = variance_selector.transform(X_test_sparse)\n",
    "selected_features = variance_selector.get_support(indices=True)\n",
    "\n",
    "for i in tqdm(range(10, 100, 10)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Apply Forward Stepwise Selection\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    forward_selector = SequentialFeatureSelector(\n",
    "        model, n_features_to_select=i, direction='forward', scoring='balanced_accuracy', cv=5\n",
    "    )\n",
    "    forward_selector.fit(X_train_var_selected, y_train)\n",
    "    selected_features = np.where(forward_selector.get_support())[0]\n",
    "\n",
    "    # Transform data\n",
    "    X_train_forward = forward_selector.transform(X_train_var_selected)\n",
    "    X_test_forward = forward_selector.transform(X_test_var_selected)\n",
    "\n",
    "    # Train and evaluate model\n",
    "    model.fit(X_train_forward, y_train)\n",
    "    y_pred = model.predict(X_test_forward)\n",
    "    runtime = time.time() - start_time\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_table = pd.concat([results_table, pd.DataFrame({\n",
    "        'Method': ['FSS'],\n",
    "        'Balanced Accuracy': [balanced_acc],\n",
    "        'Number of Features Selected': [len(selected_features)],\n",
    "        'Runtime (seconds)': [runtime],\n",
    "        'Selected Features': [selected_features.tolist()]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Variance Threshold, then Backward Stepwise Selection (BSS)\n",
    "variance_selector = VarianceThreshold(threshold=0.09)\n",
    "X_train_var_selected = variance_selector.fit_transform(X_train_sparse.toarray())\n",
    "X_test_var_selected = variance_selector.transform(X_test_sparse)\n",
    "selected_features = variance_selector.get_support(indices=True)\n",
    "\n",
    "for i in tqdm(range(10, 100, 10)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Apply Backward Stepwise Selection\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    backward_selector = SequentialFeatureSelector(\n",
    "        model, n_features_to_select=i, direction='backward', scoring='balanced_accuracy', cv=5\n",
    "    )\n",
    "    backward_selector.fit(X_train_var_selected, y_train)\n",
    "    selected_features = np.where(backward_selector.get_support())[0]\n",
    "\n",
    "    # Transform data\n",
    "    X_train_backward = backward_selector.transform(X_train_var_selected)\n",
    "    X_test_backward = backward_selector.transform(X_test_var_selected)\n",
    "\n",
    "    # Train and evaluate model\n",
    "    model.fit(X_train_backward, y_train)\n",
    "    y_pred = model.predict(X_test_backward)\n",
    "    runtime = time.time() - start_time\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_table = pd.concat([results_table, pd.DataFrame({\n",
    "        'Method': ['BSS'],\n",
    "        'Balanced Accuracy': [balanced_acc],\n",
    "        'Number of Features Selected': [len(selected_features)],\n",
    "        'Runtime (seconds)': [runtime],\n",
    "        'Selected Features': [selected_features.tolist()]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_table.to_csv(\"stepwise_selection_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) (Plotting results of FSS /BSS) omitted from report\n",
    "df = pd.read_csv(\"stepwise_selection_results.csv\")\n",
    "method_column = 'Method'\n",
    "accuracy_column = 'Balanced Accuracy'\n",
    "features_column = 'Number of Features Selected'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method, group in df.groupby(method_column):\n",
    "    plt.plot(group[features_column], group[accuracy_column], marker='', label=method)\n",
    "\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('Balanced Accuracy')\n",
    "plt.title('Figure 4: Performance of Forward and Backward Stepwise Selection')\n",
    "plt.legend(title='Method')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/figure_4.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Advanced method 1: MI + Lasso + SVC\n",
    "\n",
    "# Prepare Data\n",
    "X = csr_matrix(data_raw.iloc[:, 1:].values)\n",
    "y = data_raw.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split training set further into training and validation sets\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "best_balanced_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "# Optimize Number of Features for Mutual Information\n",
    "for k in [50, 100, 200, 500]:\n",
    "    # Mutual Information Feature Selection\n",
    "    filter_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_train_filtered = filter_selector.fit_transform(X_train_, y_train_)\n",
    "    X_val_filtered = filter_selector.transform(X_val)\n",
    "\n",
    "    # Lasso Feature Selection\n",
    "    lasso = LassoCV(\n",
    "        cv=5, random_state=42, max_iter=10000, tol=1e-4, n_alphas=100\n",
    "    ).fit(X_train_filtered.toarray(), y_train_)\n",
    "    selected_mask = lasso.coef_ != 0\n",
    "    X_train_lasso = X_train_filtered[:, selected_mask]\n",
    "    X_val_lasso = X_val_filtered[:, selected_mask]\n",
    "\n",
    "    # SVM Parameter Optimization\n",
    "    param_grid = {\"C\": [0.01, 0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\", \"poly\"]}\n",
    "    for kernel in param_grid[\"kernel\"]:\n",
    "        for C in param_grid[\"C\"]:\n",
    "            svm = SVC(C=C, kernel=kernel, class_weight=\"balanced\", random_state=42)\n",
    "            svm.fit(X_train_lasso.toarray(), y_train_)\n",
    "            y_val_pred = svm.predict(X_val_lasso.toarray())\n",
    "            balanced_acc = balanced_accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "            if balanced_acc > best_balanced_acc:\n",
    "                best_balanced_acc = balanced_acc\n",
    "                best_params = {\"k\": k, \"C\": C, \"kernel\": kernel}\n",
    "\n",
    "# Log Results\n",
    "print(\"Best Balanced Accuracy on Validation Set:\", best_balanced_acc)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# Final Evaluation on Test Set\n",
    "# Perform Mutual Information with best number of features\n",
    "filter_selector = SelectKBest(score_func=mutual_info_classif, k=best_params[\"k\"])\n",
    "X_train_final_filtered = filter_selector.fit_transform(X_train, y_train)\n",
    "X_test_final_filtered = filter_selector.transform(X_test)\n",
    "\n",
    "# Perform Lasso with selected features\n",
    "lasso = LassoCV(\n",
    "    cv=5, random_state=42, max_iter=10000, tol=1e-4, n_alphas=100\n",
    ").fit(X_train_final_filtered.toarray(), y_train)\n",
    "selected_mask = lasso.coef_ != 0\n",
    "X_train_final_lasso = X_train_final_filtered[:, selected_mask]\n",
    "X_test_final_lasso = X_test_final_filtered[:, selected_mask]\n",
    "\n",
    "# Train the final SVM model with the best parameters\n",
    "final_svm = SVC(\n",
    "    C=best_params[\"C\"], kernel=best_params[\"kernel\"], class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "final_svm.fit(X_train_final_lasso.toarray(), y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = final_svm.predict(X_test_final_lasso.toarray())\n",
    "balanced_acc_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "print(f\"Balanced Accuracy on Test Set: {balanced_acc_test}\")\n",
    "print(f\"Number of Selected Features After Lasso: {X_train_final_lasso.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Advanced method 2: Optimized random forest\n",
    "\n",
    "# Prepare Data\n",
    "X = csr_matrix(data_raw.iloc[:, 1:].values)\n",
    "y = data_raw.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split training set further for validation\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "best_balanced_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "# Optimize Number of Features for Mutual Information\n",
    "for k in [50, 100, 200, 500]:\n",
    "    # Mutual Information Feature Selection\n",
    "    filter_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_train_filtered = filter_selector.fit_transform(X_train_, y_train_)\n",
    "    X_val_filtered = filter_selector.transform(X_val)\n",
    "\n",
    "    # Random Forest Parameter Optimization\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "    }\n",
    "    for n_estimators in param_grid[\"n_estimators\"]:\n",
    "        for max_depth in param_grid[\"max_depth\"]:\n",
    "            for min_samples_split in param_grid[\"min_samples_split\"]:\n",
    "                rf_model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                )\n",
    "                rf_model.fit(X_train_filtered, y_train_)\n",
    "                y_pred_rf = rf_model.predict(X_val_filtered)\n",
    "                balanced_acc_rf = balanced_accuracy_score(y_val, y_pred_rf)\n",
    "\n",
    "                if balanced_acc_rf > best_balanced_acc:\n",
    "                    best_balanced_acc = balanced_acc_rf\n",
    "                    best_params = {\n",
    "                        \"k\": k,\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_split\": min_samples_split,\n",
    "                    }\n",
    "\n",
    "# Log Results\n",
    "print(\"Best Balanced Accuracy on Validation Set:\", best_balanced_acc)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "filter_selector = SelectKBest(score_func=mutual_info_classif, k=best_params[\"k\"])\n",
    "X_train_final_filtered = filter_selector.fit_transform(X_train, y_train)\n",
    "X_test_final_filtered = filter_selector.transform(X_test)\n",
    "\n",
    "final_rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    ")\n",
    "final_rf_model.fit(X_train_final_filtered, y_train)\n",
    "\n",
    "# Compute Feature Importance and Threshold\n",
    "feature_importances = final_rf_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  # Use mean feature importance as threshold\n",
    "selected_features_mask = feature_importances > importance_threshold\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_selected = X_train_final_filtered[:, selected_features_mask]\n",
    "X_test_selected = X_test_final_filtered[:, selected_features_mask]\n",
    "\n",
    "print(f\"Number of Selected Features After Thresholding: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Retrain and Evaluate on Test Set\n",
    "final_rf_model.fit(X_train_selected, y_train)\n",
    "y_test_pred = final_rf_model.predict(X_test_selected)\n",
    "balanced_acc_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "print(\"Final Balanced Accuracy on Test Set:\", balanced_acc_test)\n",
    "\n",
    "# Generate Heatmap of Features\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importances[selected_features_mask])[::-1]\n",
    "features_dense = X_train_selected.toarray()\n",
    "sorted_features = features_dense[:, sorted_indices]\n",
    "\n",
    "# Combine labels and sorted features\n",
    "heatmap_data = pd.DataFrame(sorted_features)\n",
    "heatmap_data.insert(0, \"label\", y_train)\n",
    "\n",
    "# Plot Heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [0.15, 0.85]}, figsize=(8, 8))\n",
    "sns.heatmap(heatmap_data[[\"label\"]], cmap='Greys', cbar=False, ax=ax1).axis(\"off\")\n",
    "sns.heatmap(heatmap_data.iloc[:, 1:], cmap='Greys', cbar=False, ax=ax2).axis(\"off\")\n",
    "\n",
    "# Add text annotations below the plots\n",
    "fig.text(0.09, -0.03, \"Label\", fontsize=14, ha=\"center\")\n",
    "fig.text(0.55, -0.03, \"Features selected by Random Forest\", fontsize=14, ha=\"center\")\n",
    "\n",
    "# Add the frame around the plot\n",
    "rect = patches.Rectangle(\n",
    "    (0, 0), 1, 1, \n",
    "    linewidth=1, edgecolor=\"black\", facecolor=\"none\", transform=fig.transFigure, figure=fig\n",
    ")\n",
    "fig.patches.append(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/heatmap_sorted_features.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Generate heatmap before Random Forest\n",
    "rest_cols = data_raw.iloc[:, 1:131]\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(rest_cols, cmap='Greys', cbar=False, ax=ax)\n",
    "ax.axis(\"off\")\n",
    "fig.text(0.5, -0.03, \"Original dataset\", fontsize=14, ha=\"center\")\n",
    "\n",
    "# Again add a frame around the plot\n",
    "rect = patches.Rectangle(\n",
    "    (0, 0), 1, 1,  \n",
    "    linewidth=1, edgecolor=\"black\", facecolor=\"none\", transform=fig.transFigure, figure=fig\n",
    ")\n",
    "fig.patches.append(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/figure_5.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
