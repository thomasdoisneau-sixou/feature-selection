{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba61570",
   "metadata": {},
   "source": [
    "# ST451 Project: Why We Use\n",
    "\n",
    "Structure\n",
    "\n",
    "1. Classification\n",
    "    1. Logistic regression (w/ penalties)\n",
    "    2. Bayesian logistic regression\n",
    "    3. GP\n",
    "    4. Summary plots\n",
    "\n",
    "2. Regression\n",
    "    1. Linear regression (w/ penalties)\n",
    "    2. Bayesian linear regression\n",
    "    3. GP\n",
    "    4. Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ef4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "from sklearn.gaussian_process import (GaussianProcessClassifier,\n",
    "                                      GaussianProcessRegressor)\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "from sklearn.linear_model import (ElasticNet, Lasso, LinearRegression,\n",
    "                                  LogisticRegression, Ridge)\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, mean_absolute_error,\n",
    "                             mean_squared_error, r2_score, roc_auc_score,\n",
    "                             roc_curve) \n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969cf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "drugs = fetch_ucirepo(id = 373)\n",
    "\n",
    "x = drugs.data.features\n",
    "y = drugs.data.targets\n",
    "\n",
    "# print(drugs.metadata)\n",
    "# print(drugs.variables)\n",
    "\n",
    "# Select features\n",
    "features = ['age', 'gender', 'education', 'nscore', 'escore', 'oscore', 'ascore', 'cscore', 'impuslive', 'ss']\n",
    "X = x[features].values\n",
    "\n",
    "# For classification: make cocaine user dummy (irregular users vs. more regular users)\n",
    "y_coke_dummy = pd.get_dummies(y['coke'] > 'CL1', drop_first=True).values.ravel()\n",
    "# y_coke_dummy = pd.get_dummies(y['coke'] != 'CL0', drop_first=True).values.ravel() # strict user vs non-user\n",
    "\n",
    "# For regression: make cocaine user score\n",
    "coke_map = {f'CL{i}': i for i in range(7)}\n",
    "y_coke_score = y['coke'].map(coke_map).values\n",
    "\n",
    "# Split data for classification task\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_coke_dummy, test_size=0.25, random_state=42)\n",
    "# For regression task\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_coke_score, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294863a7",
   "metadata": {},
   "source": [
    "## I. Classification\n",
    "\n",
    "### 1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic regression (with different penalty terms)\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define the models with different regularization\n",
    "log_reg_models = {\n",
    "    'Logistic Regression (no penalty)': LogisticRegression(penalty=None, random_state=42, max_iter=5000),\n",
    "    'Logistic Regression (L1)': LogisticRegression(penalty='l1', solver='liblinear', random_state=42, max_iter=5000),\n",
    "    'Logistic Regression (L2)': LogisticRegression(penalty='l2', random_state=42, max_iter=5000),\n",
    "    'Logistic Regression (Elastic Net)': LogisticRegression(penalty='elasticnet', solver='saga', random_state=42, max_iter=5000)\n",
    "}\n",
    "\n",
    "# Parameters for grid search\n",
    "param_grids = {\n",
    "    'Logistic Regression (no penalty)': {},\n",
    "    'Logistic Regression (L1)': {'C': np.logspace(-4, 4, 10)},\n",
    "    'Logistic Regression (L2)': {'C': np.logspace(-4, 4, 10)},\n",
    "    'Logistic Regression (Elastic Net)': {\n",
    "        'C': np.logspace(-4, 4, 5),\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fit models and evaluate\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in log_reg_models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[name], \n",
    "        cv=5, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'model': best_model\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC AUC: {auc_score:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc_score:.4f})\")\n",
    "\n",
    "# Finalize ROC curve plot\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Logistic Regression Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare coefficients across models\n",
    "plt.figure(figsize=(12, 8))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(features))\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    if hasattr(result['model'], 'coef_'):\n",
    "        coeffs = result['model'].coef_[0]\n",
    "        plt.bar(index + i*bar_width, coeffs, bar_width, label=name, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Feature Importance Comparison Across Logistic Regression Models')\n",
    "plt.xticks(index + bar_width, features, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a results comparison table\n",
    "results_table = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[name]['accuracy'] for name in results],\n",
    "    'AUC': [results[name]['auc'] for name in results]\n",
    "})\n",
    "\n",
    "# Sort by AUC score\n",
    "results_table = results_table.sort_values('AUC', ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941aa37",
   "metadata": {},
   "source": [
    "### 2. Bayesian logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Bayesian logistic regression\n",
    "\n",
    "# To run on Silicon Mac: force PyTensor to use Apple's system compiler\n",
    "pytensor.config.cxx = \"/usr/bin/clang++\"\n",
    "# Suppress some warnings to make output more readable\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Evaluate a range of priors for Bayesian logistic regression using cross-validation\n",
    "\n",
    "# Define different prior standard deviations to test\n",
    "prior_sigmas = [0.1, 0.25, 0.5, 0.75, 1, 3, 5]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "# Perform cross-validation for each prior configuration\n",
    "for sigma in prior_sigmas:\n",
    "    fold_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        # Split data for this fold\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Create and sample from model with current sigma\n",
    "        with pm.Model() as fold_model:\n",
    "            # Priors with current sigma\n",
    "            alpha = pm.Normal(\"alpha\", mu=0, sigma=3)\n",
    "            beta = pm.Normal(\"beta\", mu=0, sigma=sigma, shape=X_fold_train.shape[1])\n",
    "            \n",
    "            # Expected value of outcome\n",
    "            mu = alpha + pm.math.dot(X_fold_train, beta)\n",
    "            # Likelihood of observations\n",
    "            y_obs = pm.Bernoulli(\"y_obs\", logit_p=mu, observed=y_fold_train)\n",
    "            \n",
    "            # Sample from posterior (using a smaller number for cross-validation)\n",
    "            trace = pm.sample(500, tune=500, random_seed=42, return_inferencedata=True)\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            alpha_samples = trace.posterior.alpha.values.flatten()\n",
    "            beta_samples = trace.posterior.beta.values.reshape(-1, X_fold_train.shape[1])\n",
    "            \n",
    "            # Calculate predictions\n",
    "            logit_p = alpha_samples[:, None] + np.dot(beta_samples, X_fold_val.T)\n",
    "            probs = 1 / (1 + np.exp(-logit_p))\n",
    "            y_pred_proba = probs.mean(axis=0)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Evaluate performance\n",
    "            accuracy = (y_pred == y_fold_val).mean()\n",
    "            auc = roc_auc_score(y_fold_val, y_pred_proba)\n",
    "            fold_scores.append((accuracy, auc))\n",
    "    \n",
    "    # Average scores across folds\n",
    "    mean_accuracy = np.mean([score[0] for score in fold_scores])\n",
    "    mean_auc = np.mean([score[1] for score in fold_scores])\n",
    "    cv_results.append((sigma, mean_accuracy, mean_auc))\n",
    "    print(f\"Prior sigma = {sigma}: Accuracy = {mean_accuracy:.4f}, AUC = {mean_auc:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "sigmas, accuracies, aucs = zip(*cv_results)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sigmas, accuracies, 'o-')\n",
    "plt.xlabel('Prior Sigma')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Prior Sigma')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sigmas, aucs, 'o-')\n",
    "plt.xlabel('Prior Sigma')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC vs Prior Sigma')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose the best prior sigma based on AUC\n",
    "best_sigma_idx = np.argmax([res[2] for res in cv_results])\n",
    "best_sigma = cv_results[best_sigma_idx][0]\n",
    "print(f\"\\nBest prior sigma based on cross-validation: {best_sigma}\")\n",
    "\n",
    "# Train the model with the best sigma on the full training set and evaluate on test set\n",
    "with pm.Model() as final_model:\n",
    "    # Priors with best sigma\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=3)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=best_sigma, shape=X_train.shape[1])\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = alpha + pm.math.dot(X_train, beta)\n",
    "    # Likelihood of observations\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", logit_p=mu, observed=y_train)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    trace = pm.sample(1000, tune=1000, random_seed=42, return_inferencedata=True)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    alpha_samples = trace.posterior.alpha.values.flatten()\n",
    "    beta_samples = trace.posterior.beta.values.reshape(-1, X_train.shape[1])\n",
    "    \n",
    "    # Get mean coefficient values for plotting\n",
    "    mean_beta = beta_samples.mean(axis=0)\n",
    "    \n",
    "    # Calculate predictions on test set\n",
    "    logit_p_test = alpha_samples[:, None] + np.dot(beta_samples, X_test.T)\n",
    "    probs_test = 1 / (1 + np.exp(-logit_p_test))\n",
    "    y_pred_proba_blr = probs_test.mean(axis=0)\n",
    "    y_pred_blr = (y_pred_proba_blr > 0.5).astype(int)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy_blr = (y_pred_blr == y_test).mean()\n",
    "    auc_blr = roc_auc_score(y_test, y_pred_proba_blr)\n",
    "    print(f\"Bayesian Logistic Regression - Test Accuracy: {accuracy_blr:.4f}, AUC: {auc_blr:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(features)), mean_beta)\n",
    "plt.xticks(range(len(features)), features, rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Feature Importance - Bayesian Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add to the results dictionary\n",
    "results['Bayesian Logistic Regression'] = {'accuracy': accuracy_blr, 'auc': auc_blr}\n",
    "\n",
    "# Create an updated comparison table\n",
    "results_table = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[name]['accuracy'] for name in results],\n",
    "    'AUC': [results[name]['auc'] for name in results]\n",
    "})\n",
    "\n",
    "# Sort by AUC score in descending order\n",
    "results_table = results_table.sort_values('AUC', ascending=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f62df",
   "metadata": {},
   "source": [
    "### 3. Gaussian Process classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0771ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gaussian Process classification\n",
    "# Set up the kernel for the Gaussian Process Classifier\n",
    "kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1) # CV shows this to be the best one (not included here for brevity)\n",
    "\n",
    "# Create and train the Gaussian Process classifier\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
    "gpc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gpc = gpc.predict(X_test)\n",
    "y_pred_gpc_proba = gpc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gpc = gpc.score(X_test, y_test)\n",
    "auc_gpc = roc_auc_score(y_test, y_pred_gpc_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGaussian Process Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_gpc:.4f}\")\n",
    "print(f\"ROC AUC: {auc_gpc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gpc))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gpc))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr_gpc, tpr_gpc, _ = roc_curve(y_test, y_pred_gpc_proba)\n",
    "plt.plot(fpr_gpc, tpr_gpc, label=f'GPC (AUC = {auc_gpc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gaussian Process Classifier for Cocaine Use Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare with logistic regression models\n",
    "all_results = {\n",
    "    'Gaussian Process Classifier': {'accuracy': accuracy_gpc, 'auc': auc_gpc}\n",
    "}\n",
    "# Add logistic regression results\n",
    "for name, result in results.items():\n",
    "    all_results[name] = {'accuracy': result['accuracy'], 'auc': result['auc']}\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Model': list(all_results.keys()),\n",
    "    'Accuracy': [all_results[name]['accuracy'] for name in all_results],\n",
    "    'AUC': [all_results[name]['auc'] for name in all_results]\n",
    "})\n",
    "\n",
    "# Sort by AUC score\n",
    "comparison_table = comparison_table.sort_values('AUC', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed05d18",
   "metadata": {},
   "source": [
    "### 4. Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure feature importance for Gaussian Process Classifier\n",
    "\n",
    "# 1. Extract length scales from the RBF kernel if available\n",
    "# Try to extract length scales from the kernel\n",
    "has_length_scales = False\n",
    "try:\n",
    "    # Check if the kernel has length_scale attribute (RBF kernel)\n",
    "    if hasattr(gpc.kernel_, 'k1') and hasattr(gpc.kernel_.k1, 'length_scale'):\n",
    "        length_scales = gpc.kernel_.k1.length_scale\n",
    "        if not np.isscalar(length_scales):\n",
    "            # If we have one length scale per feature\n",
    "            has_length_scales = True\n",
    "            # Inverse of length scale can be used as importance (smaller length scale = more important)\n",
    "            kernel_importance = 1 / length_scales\n",
    "            # Normalize to 0-1 scale\n",
    "            kernel_importance = kernel_importance / kernel_importance.sum()\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.bar(range(len(features)), kernel_importance)\n",
    "            plt.xticks(range(len(features)), features, rotation=45)\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Relative Importance (from kernel length scales)')\n",
    "            plt.title('Feature Importance from GP Kernel Length Scales')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "except:\n",
    "    print(\"Could not extract length scales from the kernel.\")\n",
    "\n",
    "# 2. Calculate permutation feature importance\n",
    "# This is a model-agnostic method to determine feature importance\n",
    "result = permutation_importance(\n",
    "    gpc, X_test, y_test, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# Extract and sort importance values\n",
    "perm_importance = result.importances_mean\n",
    "perm_importance_std = result.importances_std\n",
    "sorted_idx = perm_importance.argsort()\n",
    "\n",
    "# Plot permutation importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(\n",
    "    [features[i] for i in sorted_idx], \n",
    "    perm_importance[sorted_idx],\n",
    "    xerr=perm_importance_std[sorted_idx],\n",
    "    color='cornflowerblue'\n",
    ")\n",
    "plt.xlabel('Permutation Importance (decrease in ROC AUC)')\n",
    "plt.title('Feature Importance for Gaussian Process Classifier')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance comparison for the best classifiers\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# 1. Calculate feature importance for Gaussian Process Classifier via permutation importance\n",
    "# We've already calculated this in the previous cell, so we'll reuse those values\n",
    "result = permutation_importance(\n",
    "    gpc, X_test, y_test, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "gpc_importance = result.importances_mean\n",
    "# Normalize to 0-1 scale for better comparison\n",
    "gpc_importance = gpc_importance / gpc_importance.max()\n",
    "\n",
    "# 2. Feature importance for Logistic Regression with L2 Penalty\n",
    "lr_model = results['Logistic Regression (L2)']['model']\n",
    "lr_importance = np.abs(lr_model.coef_[0])\n",
    "# Normalize \n",
    "lr_importance = lr_importance / lr_importance.max()\n",
    "\n",
    "# 3. Feature importance for Bayesian Logistic Regression\n",
    "# Extract posterior mean of beta coefficients from trace\n",
    "blr_importance = np.abs(trace.posterior.beta.mean(dim=[\"chain\", \"draw\"]).values)\n",
    "# Normalize\n",
    "blr_importance = blr_importance / blr_importance.max()\n",
    "\n",
    "# Set up bar positions\n",
    "x_pos = np.arange(len(features))\n",
    "width = 0.25\n",
    "\n",
    "# Plot the bars for each model\n",
    "plt.bar(x_pos - width, gpc_importance, width, \n",
    "        label='Gaussian Process Classifier', color='cornflowerblue', alpha=0.8)\n",
    "plt.bar(x_pos, lr_importance, width, \n",
    "        label='Logistic Regression (L2)', color='forestgreen', alpha=0.8)\n",
    "plt.bar(x_pos + width, blr_importance, width, \n",
    "        label='Bayesian Logistic Regression', color='firebrick', alpha=0.8)\n",
    "\n",
    "# Add feature names and formatting\n",
    "plt.xticks(x_pos, features, rotation=90)\n",
    "plt.ylabel('Normalized Feature Importance', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "# plt.title('Feature Importance Comparison Across Top Classification Models', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top 3 most important features for each model\n",
    "print(\"Top 3 important features - Gaussian Process Classifier:\")\n",
    "gpc_top_idx = np.argsort(gpc_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(gpc_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {gpc_importance[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 important features - Logistic Regression (L2):\")\n",
    "lr_top_idx = np.argsort(lr_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(lr_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {lr_importance[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 important features - Bayesian Logistic Regression:\")\n",
    "blr_top_idx = np.argsort(blr_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(blr_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {blr_importance[idx]:.4f}\")\n",
    "\n",
    "# Check for agreement in top features\n",
    "common_features = set([features[i] for i in gpc_top_idx]) & set([features[i] for i in lr_top_idx]) & set([features[i] for i in blr_top_idx])\n",
    "if common_features:\n",
    "    print(f\"\\nFeatures important across all three models: {', '.join(common_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ROC curves for all classifiers into one graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Add logistic regression models\n",
    "for name, result in results.items():\n",
    "    if name == 'Bayesian Logistic Regression':\n",
    "        # For Bayesian logistic regression\n",
    "        fpr_blr, tpr_blr, _ = roc_curve(y_test, y_pred_proba_blr)\n",
    "        plt.plot(fpr_blr, tpr_blr, label=f\"{name} (AUC = {auc_blr:.4f})\")\n",
    "    elif 'Logistic Regression' in name:\n",
    "        # Get the model\n",
    "        model = result['model']\n",
    "        # Calculate ROC curve\n",
    "        y_pred_proba_lr = model.predict_proba(X_test)[:, 1]\n",
    "        fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "        plt.plot(fpr_lr, tpr_lr, label=f\"{name} (AUC = {result['auc']:.4f})\")\n",
    "\n",
    "# Add Gaussian Process Classifier\n",
    "fpr_gpc, tpr_gpc, _ = roc_curve(y_test, y_pred_gpc_proba)\n",
    "plt.plot(fpr_gpc, tpr_gpc, label=f'Gaussian Process Classifier (AUC = {auc_gpc:.4f})', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "# plt.title('ROC Curves for All Classification Models', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10862c48",
   "metadata": {},
   "source": [
    "## II. Regression\n",
    "\n",
    "### 1. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear regression (with different penalty terms)\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results_reg = {}\n",
    "\n",
    "# Define the models with different regularization\n",
    "linear_reg_models = {\n",
    "    'Linear Regression (no penalty)': LinearRegression(),\n",
    "    'Linear Regression (L1)': Lasso(max_iter=10000, random_state=42),\n",
    "    'Linear Regression (L2)': Ridge(random_state=42),\n",
    "    'Linear Regression (Elastic Net)': ElasticNet(max_iter=10000, random_state=42)\n",
    "}\n",
    "\n",
    "# Parameters for grid search\n",
    "param_grids = {\n",
    "    'Linear Regression (no penalty)': {},\n",
    "    'Linear Regression (L1)': {'alpha': np.logspace(-4, 1, 10)},\n",
    "    'Linear Regression (L2)': {'alpha': np.logspace(-4, 1, 10)},\n",
    "    'Linear Regression (Elastic Net)': {\n",
    "        'alpha': np.logspace(-4, 1, 6),\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fit models and evaluate\n",
    "for name, model in linear_reg_models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    if param_grids[name]:  # If there are hyperparameters to tune\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grids[name], \n",
    "            cv=5, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        # No hyperparameters to tune\n",
    "        best_model = model\n",
    "        best_model.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results_reg[name] = {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mae': mae,\n",
    "        'model': best_model\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "# Plot actual vs predicted values for the best model based on R²\n",
    "best_model_name = max(results_reg, key=lambda x: results_reg[x]['r2'])\n",
    "best_model = results_reg[best_model_name]['model']\n",
    "best_r2 = results_reg[best_model_name]['r2']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pred_best = best_model.predict(X_test_reg)\n",
    "plt.scatter(y_test_reg, y_pred_best, alpha=0.7)\n",
    "plt.plot([0, 6], [0, 6], 'k--')\n",
    "plt.xlabel('Actual Cocaine Usage Score')\n",
    "plt.ylabel('Predicted Cocaine Usage Score')\n",
    "plt.title(f'Best Model: {best_model_name} (R² = {best_r2:.4f})')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare coefficients across models\n",
    "plt.figure(figsize=(15, 10))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(features))\n",
    "\n",
    "for i, (name, result) in enumerate(results_reg.items()):\n",
    "    if hasattr(result['model'], 'coef_'):\n",
    "        coeffs = result['model'].coef_\n",
    "        plt.bar(index + i*bar_width, coeffs, bar_width, label=name, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Feature Importance Comparison Across Linear Regression Models')\n",
    "plt.xticks(index + bar_width, features, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a results comparison table\n",
    "results_table = pd.DataFrame({\n",
    "    'Model': list(results_reg.keys()),\n",
    "    'MSE': [results_reg[name]['mse'] for name in results_reg],\n",
    "    'RMSE': [results_reg[name]['rmse'] for name in results_reg],\n",
    "    'R²': [results_reg[name]['r2'] for name in results_reg],\n",
    "    'MAE': [results_reg[name]['mae'] for name in results_reg]\n",
    "})\n",
    "\n",
    "# Sort by R² score\n",
    "results_table = results_table.sort_values('R²', ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624ca95",
   "metadata": {},
   "source": [
    "### 2. Bayesian linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1247dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Bayesian linear regression\n",
    "# Perform cross-validation to evaluate different prior settings\n",
    "\n",
    "# Define different prior configurations to test\n",
    "prior_configs = [0.1, 0.25, 0.5, 0.75, 1, 3, 5]\n",
    "\n",
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "# Perform cross-validation for each prior configuration\n",
    "for config_idx, config in enumerate(prior_configs):\n",
    "    print(f\"\\nEvaluating prior configuration {config_idx+1}/{len(prior_configs)}: {config}\")\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train_reg)):\n",
    "        # Split data for this fold\n",
    "        X_cv_train, X_cv_val = X_train_reg[train_idx], X_train_reg[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train_reg[train_idx], y_train_reg[val_idx]\n",
    "        \n",
    "        # Create and train model with current prior configuration\n",
    "        with pm.Model() as cv_model:\n",
    "            # Priors with current configuration\n",
    "            alpha = pm.Normal(\"alpha\", mu=0, sigma=5)\n",
    "            beta = pm.Normal(\"beta\", mu=0, sigma=config, shape=X_cv_train.shape[1])\n",
    "            sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "            \n",
    "            # Expected value of outcome\n",
    "            mu = alpha + pm.math.dot(X_cv_train, beta)\n",
    "            \n",
    "            # Likelihood\n",
    "            y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_cv_train)\n",
    "            \n",
    "            # Use a smaller number of samples for speed in CV\n",
    "            trace = pm.sample(500, tune=500, random_seed=42, return_inferencedata=True, progressbar=False)\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        alpha_samples = trace.posterior.alpha.values.flatten()\n",
    "        beta_samples = trace.posterior.beta.values.reshape(-1, X_cv_val.shape[1])\n",
    "        y_pred = alpha_samples[:, None] + np.dot(beta_samples, X_cv_val.T)\n",
    "        y_pred_mean = y_pred.mean(axis=0)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        mse = mean_squared_error(y_cv_val, y_pred_mean)\n",
    "        r2 = r2_score(y_cv_val, y_pred_mean)\n",
    "        fold_scores.append({'mse': mse, 'r2': r2})\n",
    "        \n",
    "        print(f\"  Fold {fold_idx+1}: MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "    \n",
    "    # Calculate average scores across folds\n",
    "    avg_mse = np.mean([score['mse'] for score in fold_scores])\n",
    "    avg_r2 = np.mean([score['r2'] for score in fold_scores])\n",
    "    cv_results.append({\n",
    "        'config': config,\n",
    "        'avg_mse': avg_mse,\n",
    "        'avg_r2': avg_r2\n",
    "    })\n",
    "    print(f\"Configuration {config_idx+1} average: MSE = {avg_mse:.4f}, R² = {avg_r2:.4f}\")\n",
    "\n",
    "# Find the best configuration\n",
    "best_config_idx = np.argmax([result['avg_r2'] for result in cv_results])\n",
    "best_config = prior_configs[best_config_idx]\n",
    "print(f\"\\nBest prior configuration: {best_config}\")\n",
    "print(f\"Best average R²: {cv_results[best_config_idx]['avg_r2']:.4f}\")\n",
    "print(f\"Best average MSE: {cv_results[best_config_idx]['avg_mse']:.4f}\")\n",
    "\n",
    "# Use the best configuration for the final model\n",
    "print(\"\\nTraining final model with best configuration...\")\n",
    "# Set up the Bayesian linear regression model\n",
    "with pm.Model() as blr_reg_model:\n",
    "    # Priors for unknown model parameters\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=3)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=best_config, shape=X_train_reg.shape[1])\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = alpha + pm.math.dot(X_train_reg, beta)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train_reg)\n",
    "    \n",
    "    # Sample from the posterior\n",
    "    trace_reg = pm.sample(1000, tune=1000, random_seed=42, return_inferencedata=True)\n",
    "    \n",
    "    # Summary of the posterior\n",
    "    summary_reg = az.summary(trace_reg, var_names=[\"alpha\", \"beta\", \"sigma\"])\n",
    "    print(\"\\nBayesian Linear Regression - Parameter Summary (truncated):\")\n",
    "    print(summary_reg.head())\n",
    "\n",
    "# Create function to predict from the Bayesian linear regression model\n",
    "def predict_blr(X, trace):\n",
    "    # Extract samples\n",
    "    alpha_samples = trace.posterior.alpha.values.flatten()\n",
    "    beta_samples = trace.posterior.beta.values.reshape(-1, X.shape[1])\n",
    "    \n",
    "    # Calculate predictions\n",
    "    y_pred = alpha_samples[:, None] + np.dot(beta_samples, X.T)\n",
    "    return y_pred.mean(axis=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_blr_reg = predict_blr(X_test_reg, trace_reg)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_blr = mean_squared_error(y_test_reg, y_pred_blr_reg)\n",
    "rmse_blr = np.sqrt(mse_blr)\n",
    "r2_blr = r2_score(y_test_reg, y_pred_blr_reg)\n",
    "mae_blr = mean_absolute_error(y_test_reg, y_pred_blr_reg)\n",
    "\n",
    "print(\"\\nBayesian Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error: {mse_blr:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse_blr:.4f}\")\n",
    "print(f\"R² Score: {r2_blr:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_blr:.4f}\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_reg, y_pred_blr_reg, alpha=0.7)\n",
    "plt.plot([0, 6], [0, 6], 'k--')\n",
    "plt.xlabel('Actual Cocaine Usage Score')\n",
    "plt.ylabel('Predicted Cocaine Usage Score')\n",
    "plt.title('Bayesian Linear Regression: Actual vs Predicted Cocaine Usage Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance (coefficients)\n",
    "plt.figure(figsize=(12, 6))\n",
    "coef_means_blr = trace_reg.posterior.beta.mean(dim=[\"chain\", \"draw\"]).values\n",
    "plt.bar(range(len(features)), coef_means_blr)\n",
    "plt.xticks(range(len(features)), features, rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Feature Importance - Bayesian Linear Regression')\n",
    "plt.show()\n",
    "\n",
    "# Add to the results dictionary\n",
    "results_reg['Bayesian Linear Regression'] = {'mse': mse_blr, 'rmse': rmse_blr, 'r2': r2_blr, 'mae': mae_blr}\n",
    "\n",
    "# Create an updated comparison table\n",
    "comparison_table_reg_updated = pd.DataFrame({\n",
    "    'Model': list(results_reg.keys()),\n",
    "    'MSE': [results_reg[name]['mse'] for name in results_reg],\n",
    "    'RMSE': [results_reg[name]['rmse'] for name in results_reg],\n",
    "    'R²': [results_reg[name]['r2'] for name in results_reg],\n",
    "    'MAE': [results_reg[name]['mae'] for name in results_reg]\n",
    "})\n",
    "\n",
    "# Sort by R² score in descending order\n",
    "comparison_table_reg_updated = comparison_table_reg_updated.sort_values('R²', ascending=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_table_reg_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8629a",
   "metadata": {},
   "source": [
    "### 3. Gaussian Process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5874ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gaussian Process regression\n",
    "# Set up the kernel for Gaussian Process Regression\n",
    "# Using a combination of RBF kernel with a constant kernel and white noise (again, found by CV, omitted here for brevity)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(0.1)\n",
    "\n",
    "# Create and train the Gaussian Process regressor\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "gpr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gpr = gpr.predict(X_test_reg)\n",
    "y_pred_gpr_std = np.sqrt(gpr.predict(X_test_reg, return_std=True)[1])\n",
    "\n",
    "# Evaluate the model\n",
    "mse_gpr = mean_squared_error(y_test_reg, y_pred_gpr)\n",
    "rmse_gpr = np.sqrt(mse_gpr)\n",
    "r2_gpr = r2_score(y_test_reg, y_pred_gpr)\n",
    "mae_gpr = mean_absolute_error(y_test_reg, y_pred_gpr)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGaussian Process Regressor Results:\")\n",
    "print(f\"Mean Squared Error: {mse_gpr:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse_gpr:.4f}\")\n",
    "print(f\"R² Score: {r2_gpr:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_gpr:.4f}\")\n",
    "\n",
    "# Plot actual vs predicted values with confidence intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(y_test_reg, y_pred_gpr, yerr=1.96*y_pred_gpr_std, fmt='o', alpha=0.5, ecolor='lightgray', label='Prediction with 95% CI')\n",
    "plt.plot([0, 6], [0, 6], 'k--', label='Perfect prediction')\n",
    "plt.xlabel('Actual Cocaine Usage Score')\n",
    "plt.ylabel('Predicted Cocaine Usage Score')\n",
    "plt.title('Gaussian Process Regression: Actual vs Predicted Cocaine Usage Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare with other regression models\n",
    "all_results_reg = {\n",
    "    'Gaussian Process Regressor': {'mse': mse_gpr, 'rmse': rmse_gpr, 'r2': r2_gpr, 'mae': mae_gpr}\n",
    "}\n",
    "# Add existing linear regression results\n",
    "for name, result in results_reg.items():\n",
    "    all_results_reg[name] = {'mse': result['mse'], 'rmse': result['rmse'], 'r2': result['r2'], 'mae': result['mae']}\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_table_reg = pd.DataFrame({\n",
    "    'Model': list(all_results_reg.keys()),\n",
    "    'MSE': [all_results_reg[name]['mse'] for name in all_results_reg],\n",
    "    'RMSE': [all_results_reg[name]['rmse'] for name in all_results_reg],\n",
    "    'R²': [all_results_reg[name]['r2'] for name in all_results_reg],\n",
    "    'MAE': [all_results_reg[name]['mae'] for name in all_results_reg]\n",
    "})\n",
    "\n",
    "# Sort by R² score\n",
    "comparison_table_reg = comparison_table_reg.sort_values('R²', ascending=False)\n",
    "\n",
    "print(\"\\nRegression Model Performance Comparison:\")\n",
    "print(comparison_table_reg)\n",
    "\n",
    "# Examine the optimized kernel parameters\n",
    "print(\"\\nOptimized kernel parameters:\")\n",
    "print(gpr.kernel_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f570c",
   "metadata": {},
   "source": [
    "### 4. Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual vs predicted values for Gaussian Process Regression and Bayesian Linear Regression\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# For Bayesian Linear Regression, use the predictions we already calculated earlier\n",
    "y_pred_blr = y_pred_blr_reg  \n",
    "\n",
    "# Plot for Gaussian Process Regressor\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_reg, y_pred_gpr, alpha=0.7, color='cornflowerblue')\n",
    "plt.plot([0, 6], [0, 6], 'k--')\n",
    "plt.xlabel('Actual Cocaine Usage Score')\n",
    "plt.ylabel('Predicted Cocaine Usage Score')\n",
    "plt.title(f'Gaussian Process Regressor (R² = {r2_gpr:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot for Bayesian Linear Regression\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_reg, y_pred_blr, alpha=0.7, color='forestgreen')\n",
    "plt.plot([0, 6], [0, 6], 'k--')\n",
    "plt.xlabel('Actual Cocaine Usage Score')\n",
    "plt.ylabel('Predicted Cocaine Usage Score')\n",
    "plt.title(f'Bayesian Linear Regression (R² = {results_reg[\"Bayesian Linear Regression\"][\"r2\"]:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate prediction errors and visualize their distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Errors for Gaussian Process Regressor\n",
    "errors_gpr = y_test_reg - y_pred_gpr\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors_gpr, bins=20, alpha=0.7, color='cornflowerblue')\n",
    "plt.axvline(x=0, color='k', linestyle='--')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Gaussian Process Regressor Error Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Errors for Bayesian Linear Regression\n",
    "errors_blr = y_test_reg - y_pred_blr\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(errors_blr, bins=20, alpha=0.7, color='forestgreen')\n",
    "plt.axvline(x=0, color='k', linestyle='--')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bayesian Linear Regression Error Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics of the errors\n",
    "print(\"Gaussian Process Regressor Error Statistics:\")\n",
    "print(f\"Mean Error: {np.mean(errors_gpr):.4f}\")\n",
    "print(f\"Std Dev: {np.std(errors_gpr):.4f}\")\n",
    "print(f\"Min: {np.min(errors_gpr):.4f}, Max: {np.max(errors_gpr):.4f}\")\n",
    "print()\n",
    "print(\"Bayesian Linear Regression Error Statistics:\")\n",
    "print(f\"Mean Error: {np.mean(errors_blr):.4f}\")\n",
    "print(f\"Std Dev: {np.std(errors_blr):.4f}\")\n",
    "print(f\"Min: {np.min(errors_blr):.4f}, Max: {np.max(errors_blr):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e729b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "\n",
    "# Create a comparison plot of feature importance for the three best regression models\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Extract feature importance for Gaussian Process Regression\n",
    "# Using permutation importance\n",
    "result_gpr = permutation_importance(\n",
    "    gpr, X_test_reg, y_test_reg, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    scoring='r2'\n",
    ")\n",
    "perm_importance_gpr = result_gpr.importances_mean\n",
    "gpr_importance = perm_importance_gpr\n",
    "# Normalize to 0-1 scale for better comparison\n",
    "gpr_importance = gpr_importance / gpr_importance.max()\n",
    "\n",
    "# 2. Extract feature importance for Linear Regression with L2 Penalty\n",
    "lr_l2_model = results_reg['Linear Regression (L2)']['model']\n",
    "lr_importance = np.abs(lr_l2_model.coef_)\n",
    "# Normalize\n",
    "lr_importance = lr_importance / lr_importance.max()\n",
    "\n",
    "# 3. Extract feature importance for Bayesian Linear Regression\n",
    "blr_importance = np.abs(coef_means_blr)\n",
    "# Normalize\n",
    "blr_importance = blr_importance / blr_importance.max()\n",
    "\n",
    "# Set up the bar chart with vertical bars\n",
    "x_pos = np.arange(len(features))\n",
    "width = 0.25\n",
    "\n",
    "# Plot bars for each model\n",
    "plt.bar(x_pos - width, gpr_importance, width, \n",
    "         label='Gaussian Process Regression', color='cornflowerblue', alpha=0.8)\n",
    "plt.bar(x_pos, lr_importance, width, \n",
    "         label='Linear Regression (L2)', color='forestgreen', alpha=0.8)\n",
    "plt.bar(x_pos + width, blr_importance, width, \n",
    "         label='Bayesian Linear Regression', color='firebrick', alpha=0.8)\n",
    "\n",
    "# Add feature names, title, and labels\n",
    "plt.xticks(x_pos, features, rotation=90)\n",
    "plt.ylabel('Normalized Feature Importance', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "# plt.title('Feature Importance Comparison Across Top Regression Models', fontsize=16)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Add grid lines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add a tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the top 3 most important features for each model\n",
    "print(\"Top 3 important features - Gaussian Process Regression:\")\n",
    "gpr_top_idx = np.argsort(gpr_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(gpr_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {gpr_importance[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 important features - Linear Regression (L2):\")\n",
    "lr_top_idx = np.argsort(lr_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(lr_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {lr_importance[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 important features - Bayesian Linear Regression:\")\n",
    "blr_top_idx = np.argsort(blr_importance)[-3:]\n",
    "for i, idx in enumerate(reversed(blr_top_idx)):\n",
    "    print(f\"{i+1}. {features[idx]}: {blr_importance[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
